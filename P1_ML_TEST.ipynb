{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "3020295474a829f1fb495c31a85967e845fec65ed3844e593a9decb777626608"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import io\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_dataset = io.loadmat('p1_dataset.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(8400, 2048)\n",
      "(8400,)\n",
      "(8400, 2048)\n",
      "(8400,)\n"
     ]
    }
   ],
   "source": [
    "# label : 1 (p1)\n",
    "p1 = p1_dataset['p1_noise']\n",
    "print(p1.shape)\n",
    "p1_re = np.real(p1)\n",
    "p1_im = np.imag(p1)\n",
    "X_p1 = np.stack((p1_re,p1_im),axis=1)\n",
    "y_p1 = np.ones(len(p1),dtype=int)\n",
    "print(y_p1.shape)\n",
    "# label : 0 (noise)\n",
    "n = p1_dataset['noise_data']\n",
    "print(n.shape)\n",
    "n_re = np.real(n)\n",
    "n_im = np.imag(n)\n",
    "X_re = np.stack((p1_re,p1_im),axis=1)\n",
    "y_re = np.zeros(len(n),dtype=int)\n",
    "print(y_re.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(16800, 2, 2048)\n(16800,)\n"
     ]
    }
   ],
   "source": [
    "X = np.vstack((X_p1,X_re))\n",
    "y = np.concatenate((y_p1,y_re),axis=None)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "11760 5040\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "x_tensor = torch.from_numpy(X).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor,y_tensor)\n",
    "dataset_size = len(dataset)\n",
    "train_split = 0.7\n",
    "train_size = int(train_split * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size,test_size])\n",
    "print(len(train_dataset), len(val_dataset))\n",
    "# train_loader = DataLoader(dataset=train_dataset,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "test_loader = DataLoader(dataset=val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "# Hyper-parameter\n",
    "learning_rate = 0.001\n",
    "batch_size = 400\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_Classifier(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(720, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.unsqueeze(x, dim=1) \n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(x.shape[0],-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model = LSTM(input_size, hidden_size, layer_num,dropout, num_classes, sequence_length,device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(LSTM_model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path=PATH, trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, batch_size, patience, n_epochs, train_loader , test_loader, PATH, device):\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    avg_train_losses = []\n",
    "    avg_valid_losses = []\n",
    "    early_stopping = EarlyStopping(patience= patience, verbose=True, path=PATH+'checkpoint.pt')\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            #data = F.normalize(data, p=2)\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            #print(data.shape)\n",
    "            \n",
    "            # Pytorch Dataloader data output shape : Batch x 1 x Data shape\n",
    "            \n",
    "            # Required data shape for RNN : Batch x Sequence Lengh x Data shape\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            scores = model(data)\n",
    "\n",
    "            loss = criterion(scores, torch.argmax(targets, dim=1))\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "        for data , target in test_loader :\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            # forward pass: 입력된 값을 모델로 전달하여 예측 출력 계산\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, torch.argmax(target, dim=1))\n",
    "            # record validation loss\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "        # print 학습/검증 statistics\n",
    "        # epoch당 평균 loss 계산\n",
    "        train_loss = np.average(train_losses)\n",
    "        valid_loss = np.average(valid_losses)\n",
    "        avg_train_losses.append(train_loss)\n",
    "        avg_valid_losses.append(valid_loss)\n",
    "\n",
    "        epoch_len = len(str(n_epochs))\n",
    "\n",
    "\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "\n",
    "        print(print_msg)\n",
    "\n",
    "        # clear lists to track next epoch\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        # early_stopping는 validation loss가 감소하였는지 확인이 필요하며,\n",
    "        # 만약 감소하였을경우 현제 모델을 checkpoint로 만든다.\n",
    "        early_stopping(valid_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "   # best model이 저장되어있는 last checkpoint를 로드한다.\n",
    "    model.load_state_dict(torch.load(PATH + 'checkpoint.pt'))\n",
    "\n",
    "    return  model, avg_train_losses, avg_valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 20\n",
    "model, train_loss, valid_loss = train_model(LSTM_model, batch_size, patience, num_epochs, train_loader, test_loader, PATH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련이 진행되는 과정에 따라 loss를 시각화\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(range(1,len(train_loss)+1),train_loss, label='Training Loss')\n",
    "plt.plot(range(1,len(valid_loss)+1),valid_loss,label='Validation Loss')\n",
    "\n",
    "# validation loss의 최저값 지점을 찾기\n",
    "minposs = valid_loss.index(min(valid_loss))+1\n",
    "plt.axvline(minposs, linestyle='--', color='r',label='Early Stopping Checkpoint')\n",
    "\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "#plt.ylim(1,3) # 일정한 scale\n",
    "#plt.xlim(0, len(train_loss)+1) # 일정한 scale\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(PATH + 'loss_plot.png', bbox_inches = 'tight')"
   ]
  }
 ]
}